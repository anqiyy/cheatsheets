\documentclass[7pt,landscape]{article}
\usepackage{amsmath, amssymb, amsthm}


% \usepackage[margin=0.1in]{geometry}
\usepackage[
    total={130mm,277mm},
    top=0mm,
    bottom=0mm,
    left=0mm,
    marginparwidth=0mm,
    marginparsep=0mm,
    centering,
    includefoot]{geometry}
\usepackage{multicol}
\usepackage{booktabs}
\setlength{\columnsep}{4pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}
\usepackage{enumitem}
\setlist{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlist[itemize]{leftmargin=*}

\renewcommand{\baselinestretch}{0.8}

\begin{document}

\scriptsize

    

    \fbox{\begin{minipage}[t]{0.333\textwidth}
    \textbf{Realizable (Separable) case}
    When there is a model that separates perfectly.
    There is \( h^* \in \mathcal{H} \) that achieves perfect separation between classes, i.e. zero true loss:  
        $ L_({\mathcal{D},f})(h^*) = 0$, in-sample loss \( L_S(h^*) = 0 \)
    
    \textbf{Agnostic case}
        When there is no perfect separator, find the best imperfect one
    \end{minipage}
    }
    \fbox{
        \begin{minipage}[t]{0.333\textwidth}
        \textbf{Empirical loss/risk }
        of any hypothesis \( h \in H \) as
    $ L_S(h) \overset{\text{def}}{=} \frac{|\{i \in [m] : h(x_i) \neq y_i\}|}{m}$
        \end{minipage}
    }
    \fbox{
        \begin{minipage}[t]{0.333\textwidth}
        
        \textbf{Assumption (iid):} Examples are independent and identically distributed according to $\mathcal{D}$ ($S \sim \mathcal{D}^m$)
        \textbf{ERM Algorithm A:} Check all $h \in \mathcal{H}$
        , Pick $h_S = \operatorname*{argmin}_{h \in \mathcal{H}} L_S(h)$. 
      This $h_S$ is the best we can do with the data, but may not be perfect on unseen data
        \end{minipage}
    }   
    \fbox{
        \begin{minipage}[t]{0.333\textwidth}
        
        \textbf{Sampling Bound in Realizable Finite Case}
        With assumptions of realizability and finite $\mathcal{H}$:
        \[ m \geq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} \]
        suffices for $\epsilon$, $\delta$ guarantee:
        \[ \mathbb{P}[L_{D,f}(h_S) \leq \epsilon] \geq 1 - \delta \]
        
        \end{minipage}
    }
    \fbox{
        \begin{minipage}[t]{0.333\textwidth}
            For $0 < p < 1$: $(1 - p)^{\frac{1}{p}} \leq \frac{1}{e}$
            
        $ P(A \cup B) \leq P(A) + P(B) $
        \end{minipage}
    }
    \fbox{
        \begin{minipage}[t]{0.333\textwidth}
        \textbf{PAC Learnability}
            \\ A hypothesis class $\mathcal{H}$ is PAC learnable if:
            \begin{itemize}
                \item There exists a function $m_{\mathcal{H}}:(0,1)^2 \to \mathbb{N}$
                \item And an algorithm that for every $\epsilon, \delta$, distribution $D$ over $X$:
                \begin{itemize}
                    \item With realizability assumption
                    \item On $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d samples
                    \item Finds $h$ satisfying $L(D,f)(h) \leq \epsilon$
                    \item With probability $\geq 1 - \delta$
                \end{itemize}
            \end{itemize}
        
        \end{minipage}
    }
    \fbox{
        \begin{minipage}[t]{0.333\textwidth}
        \textbf{Agnostic PAC Learnability}
            \begin{itemize}
                \item Modified data generating distribution: $D$ over $X \times Y$
                \item True risk: \\ $L_D(h) \overset{\text{def}}{=} \mathbb{P}_{(x,y) \sim D}[h(x) \neq y]$
                \item Finds $h$ satisfying \\ $L_D(h) \leq \min_{h' \in \mathcal{H}} L_D(h') + \epsilon$
                \item With probability $\geq 1 - \delta$
            \end{itemize}
        
        \end{minipage}
    }
    \fbox{
    \begin{minipage}[t]{0.333\textwidth}
        \textbf{Generalized Loss}
        \begin{itemize}
            \item Domain $Z$, loss function $\ell: \mathcal{H} \times Z \rightarrow \mathbb{R}_+$
            \item True risk: $L_D(h) = \mathbb{E}_{z \sim D}[\ell(h, z)]$
            \item Empirical risk: $L_S(h) = \frac{1}{m} \sum_{i=1}^m \ell(h, z_i)$
        \end{itemize}
    \end{minipage}
    }
    \fbox{
    \begin{minipage}[t]{0.333\textwidth}
        \textbf{Representative Samples}
        \\ $S$ is $\epsilon$-representative w.r.t $(Z, \mathcal{H}, D)$ if:
        \[ \forall h \in \mathcal{H}, |L_S(h) - L_D(h)| \leq \epsilon \]
        Performance on $S$ matches general performance on $D$

    \end{minipage}
    }
    \fbox{
    \begin{minipage}[t]{0.333\textwidth}
        \textbf{Representative Samples}
        \\ $S$ is $\epsilon$-representative w.r.t $(Z, \mathcal{H}, D)$ if:
        \[ \forall h \in \mathcal{H}, |L_S(h) - L_D(h)| \leq \epsilon \]

    \end{minipage}
    }
    


\end{document}